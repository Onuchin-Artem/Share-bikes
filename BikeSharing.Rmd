---
title: "Bike sharing"
output: pdf_document
---
Load default libraries
----------------------
```{r, message=F, warning=F}
library(data.table)
library(dplyr)
library(lubridate)
library(ggplot2)
library(caret)
library(stringi)
library(xgboost)
library(reshape2)
theme_set(theme_bw())
set.seed(42)
```
Read the data
-------------
```{r, message=F, warning=F}
full_train = fread("./train.csv", header = T, sep = ",", integer64 = "numeric")
test = fread("./test.csv", header = T, sep = ",", integer64 = "numeric")
```

Change type to categorical variables
------------------------------------
```{r}
fix_types = function(data) {
  data %>% 
    mutate(
      datetime = ymd_hms(datetime),
      workingday = as.character(workingday),
      weather = as.character(weather)) %>%
    mutate(
      year = as.character(year(datetime)),
      month = as.character(month(datetime)),
      day = as.character(mday(datetime)),
      wday = as.character(wday(datetime)),
      hour = as.character(hour(datetime)))
  
}
full_train = fix_types(full_train)
test = fix_types(test)

train = full_train[as.integer(full_train$day) < 13]
validate = full_train[as.integer(full_train$day) >= 13]

```

Prepare features
----------------
```{r, echo=FALSE}

dummy = dummyVars(~ workingday + weather + month + wday + hour + year,
                  data = train,
                  fullRank = T,
                  levelsOnly = T)

summarise_stats = function(groups) {
  groups %>% summarise(
    avg_count = median(as.double(count)),
    casual_ratio1 = sum(casual) / sum(count),
    casual_ratio2 = mean(casual / count))
}

train_hour_statistics = train %>% group_by(hour) %>% summarise_stats()
full_train_hour_statistics = full_train %>% group_by(hour) %>% summarise_stats()
train_wday_statistics = train %>% group_by(wday) %>% summarise_stats()
full_train_wday_statistics = full_train %>% group_by(wday) %>% summarise_stats()
train_month_statistics = train %>% group_by(month) %>% summarise_stats()
full_train_month_statistics = full_train %>% group_by(month) %>% summarise_stats()


names = names(predict(dummy, train) %>% as.data.frame())
prepare_dataset_features = function(data, hour_statistics, wday_statistics, month_statistics) {
  categorical_data = predict(dummy, data) %>% as.data.frame()
  Missing = setdiff(names, names(categorical_data))  # Find names of missing columns
  categorical_data[Missing] = 0                    # Add them, filled with '0's
  categorical_data = categorical_data[names]
  numeric_data = data %>% transmute(temp, atemp - temp, humidity, windspeed)
  hour_stats_data = inner_join(data, hour_statistics, by="hour") %>%
    select(avg_count:casual_ratio2)
  wday_stats_data = inner_join(data, wday_statistics, by="wday") %>%
    select(avg_count:casual_ratio2)
  month_stats_data = inner_join(data, month_statistics, by="month") %>%
    select(avg_count:casual_ratio2)

  prepared_data = cbind(
    categorical_data,
    numeric_data,
    hour_stats_data,
    wday_stats_data,
    month_stats_data)
  as.matrix(prepared_data)
}

train_matrix = prepare_dataset_features(train, 
                                        train_hour_statistics, 
                                        train_wday_statistics, 
                                        train_month_statistics)
validate_matrix = prepare_dataset_features(validate, 
                                           train_hour_statistics, 
                                           train_wday_statistics, 
                                           train_month_statistics)

test_matrix = prepare_dataset_features(test,
                                       full_train_hour_statistics, 
                                       full_train_wday_statistics, 
                                       full_train_month_statistics)
full_train_matrix = prepare_dataset_features(full_train,
                                             full_train_hour_statistics, 
                                             full_train_wday_statistics,
                                             full_train_month_statistics)
```

Prepare target
--------------
We going to predict log(Y+1) to optimize the target cost function
```{r}
preparedTrainTarget = log(train$count + 1)
preparedValidateTarget = log(validate$count + 1)
preparedFullTarget =log(full_train$count + 1)

```

Xgboost train and cross-validate
--------------------------------
Interesting link: [how to tune hyperparameters](http://www.slideshare.net/odsc/owen-zhangopen-sourcetoolsanddscompetitions1)
```{r}
dtrain <- xgb.DMatrix(train_matrix, label = preparedTrainTarget)

xgbControl = list(
   subsample=0.8, colsample_bytree = 0.8, metrics=list("rmse"), gamma = 0.9,
                  max.depth = 6, eta = 0.1, alpha = 1, lambda = 1, objective = "reg:linear"
)
model = xgboost(params = xgbControl, data = dtrain, 
                nround=2000, nthread = 4, print.every.n = 500)
history <- xgb.cv(params = xgbControl, data = dtrain, 
                  nround=2000, nthread = 4, nfold = 10, print.every.n = 500)
print(tail(history))

```
Validate Score
------------------------
```{r}
validate_predictions = predict(model, validate_matrix)
RMSE(validate_predictions, preparedValidateTarget)

```
Plot learning curve
-------------------
```{r}
plot_learning_curve = function(learning_curves) {
  learning_curves_train = NULL
  learning_curves_train$dataset = 'train'
  learning_curves_train$rmse = learning_curves$train.rmse.mean
  learning_curves_train$rmse.se = learning_curves$train.rmse.std
  learning_curves_train$iterations = 1:nrow(learning_curves)
  learning_curves_test = NULL
  learning_curves_test$dataset = 'test'
  learning_curves_test$rmse = learning_curves$test.rmse.mean
  learning_curves_test$rmse.se = learning_curves$test.rmse.std
  learning_curves_test$iterations = 1:nrow(learning_curves)
  learning_curves_prepared = rbind(as.data.frame(learning_curves_train), as.data.frame(learning_curves_test))
  ggplot(data = learning_curves_prepared,
         mapping = aes(x=iterations, y=rmse, group = dataset, colour=dataset)) +
    geom_errorbar(aes(ymin=rmse-2*rmse.se, ymax=rmse+2*rmse.se), width=.01, alpha=0.02) +
    geom_line() + coord_cartesian(ylim = c(0.3, 0.7))

}

plot_learning_curve(history)
```

Feature importance
------------------
```{r}
imp = xgb.importance(colnames(train_matrix), model = model)
```

```{r, fig.width=4, fig.height=7}
xgb.plot.importance(imp)

```

Train on full dataset
------------------------
```{r}
dtrain_final <- xgb.DMatrix(full_train_matrix, label = preparedFullTarget)

model_final = xgboost(params = xgbControl, data = dtrain_final, 
                      nround=2000, nthread = 4, print.every.n = 500)

history <- xgb.cv(params = xgbControl, data = dtrain_final, 
                  nround=2000, nthread = 4, print.every.n = 500, nfold = 10)

```


Predict and un-log predictions
------------------------------

```{r}
predictions = predict(model_final, test_matrix)
fixed_predictions = exp(predictions) - 1
```

Write the result
----------------
```{r}
result = cbind(as.character(test$datetime), fixed_predictions) %>% as.data.frame()
names(result) = c('datetime', 'count')
write.csv(result, 'submission.csv', quote = F, row.names = F)
```