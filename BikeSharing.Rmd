---
title: "Bike sharing"
output: pdf_document
---
Load default libraries
----------------------
```{r, message=F, warning=F}
library(data.table)
library(dplyr)
library(lubridate)
library(ggplot2)
library(caret)
library(stringi)
library(xgboost)
library(reshape2)
theme_set(theme_bw())
set.seed(42)
```
Read the data
-------------
```{r, message=F, warning=F}
full_train = fread("./train.csv", header = T, sep = ",", integer64 = "numeric")
test = fread("./test.csv", header = T, sep = ",", integer64 = "numeric")
```

Change type to categorical variables
------------------------------------
```{r}
fix_types = function(data) {
  data %>% 
    mutate(
      datetime = ymd_hms(datetime),
      workingday = 1 - workingday,
      weather = weather) %>%
    mutate(
      year = year(datetime),
      month = month(datetime),
      day = as.character(mday(datetime)),
      wday = wday(datetime),
      hour = hour(datetime),
      hour4 = as.character(round(hour(datetime) / 4)),
      hour3 = as.character(round(hour(datetime) / 3)),
      hour6 = as.character(round(hour(datetime) / 6)),
      hour8 = as.character(round((hour(datetime) - 4) / 8)),
      hour12 = as.character(round((hour(datetime)) / 12)),
      month2  = as.character(round((month(datetime)) / 2)))
}
full_train = fix_types(full_train)
test = fix_types(test)

train = full_train[as.integer(full_train$day) < 13]
validate = full_train[as.integer(full_train$day) >= 13]

```

Prepare features
----------------
```{r}

dummy = dummyVars(~ workingday + season + weather + wday + hour + 
                    year + hour4 + hour8 + hour12 + hour3 + hour6 + holiday + 
                    workingday * hour4 + workingday * hour8 + workingday * hour12 + 
                    workingday * hour3 + workingday * hour6 +
                    holiday * hour4 + holiday * hour8 + holiday * hour12 + 
                    holiday * hour3 + holiday * hour6,
                  data = train,
                  fullRank = T,
                  levelsOnly = T)

summarise_stats = function(groups) {
  groups %>% summarise(
    avg_count = median(as.double(count)),
    var_count = var(as.double(count)),
    var_registered = var(as.double(registered)),
    casual_ratio1 = sum(casual) / sum(count),
    casual_ratio2 = mean(casual / count))
}

train_hour_statistics = train %>% group_by(hour, wday) %>% summarise_stats()
full_train_hour_statistics = full_train %>% group_by(hour, wday) %>% summarise_stats()
train_month_statistics = train %>% group_by(month, year) %>% summarise_stats()
full_train_month_statistics = full_train %>% group_by(month, year) %>% summarise_stats()


select_statistics = function(data, suffix) {
    stats = data %>% select(avg_count:casual_ratio2) 
    names(stats) = paste0(names(stats), suffix)
    stats
}

names = names(predict(dummy, train) %>% as.data.frame())
prepare_dataset_features = function(data, hour_statistics, month_statistics) {
  categorical_data = predict(dummy, data) %>% as.data.frame()
  Missing = setdiff(names, names(categorical_data))  # Find names of missing columns
  categorical_data[Missing] = 0                    # Add them, filled with '0's
  categorical_data = categorical_data[names]
  numeric_data = data %>% transmute(temp, atemp - temp, humidity, windspeed)
  hour_stats_data = inner_join(data, hour_statistics, by=c("hour", "wday"), p) %>%
    select_statistics(".hour")
  month_stats_data = inner_join(data, month_statistics, by=c("month", "year")) %>%
    select_statistics(".month")

  prepared_data = cbind(
    categorical_data,
    numeric_data,
    hour_stats_data,
    month_stats_data)
  as.matrix(prepared_data)
}

train_matrix = prepare_dataset_features(train, 
                                        train_hour_statistics, 
                                        train_month_statistics)
validate_matrix = prepare_dataset_features(validate, 
                                           train_hour_statistics, 
                                           train_month_statistics)

test_matrix = prepare_dataset_features(test,
                                       full_train_hour_statistics, 
                                       full_train_month_statistics)
full_train_matrix = prepare_dataset_features(full_train,
                                             full_train_hour_statistics, 
                                             full_train_month_statistics)
```

Prepare target
--------------
We going to predict log(Y+1) to optimize the target cost function
```{r}
preparedTrainTarget = log(train$count + 1)

preparedValidateTarget = log(validate$count + 1)
preparedFullTarget =log(full_train$count + 1)

```

Xgboost train and cross-validate
--------------------------------
Interesting link: [how to tune hyperparameters](http://www.slideshare.net/odsc/owen-zhangopen-sourcetoolsanddscompetitions1)
```{r}
dtrain <- xgb.DMatrix(train_matrix, label = preparedTrainTarget)

xgbControl = list(
   subsample=0.8, colsample_bytree = 0.8, metrics=list("rmse"), gamma = 0.9,
                  max.depth = 6, eta = 0.11, alpha = 1, lambda = 1, objective = "reg:linear"
)
model = xgboost(params = xgbControl, data = dtrain, 
                nround=2000, nthread = 4, print.every.n = 500)
history <- xgb.cv(params = xgbControl, data = dtrain, 
                  nround=2000, nthread = 4, nfold = 10, print.every.n = 500)
print(tail(history))

```
Validate Score
------------------------
```{r}
validate_predictions = predict(model, validate_matrix)
RMSE(validate_predictions, preparedValidateTarget)
```



Plot learning curve
-------------------
```{r}
plot_learning_curve = function(learning_curves) {
  learning_curves_train = NULL
  learning_curves_train$dataset = 'train'
  learning_curves_train$rmse = learning_curves$train.rmse.mean
  learning_curves_train$rmse.se = learning_curves$train.rmse.std
  learning_curves_train$iterations = 1:nrow(learning_curves)
  learning_curves_test = NULL
  learning_curves_test$dataset = 'test'
  learning_curves_test$rmse = learning_curves$test.rmse.mean
  learning_curves_test$rmse.se = learning_curves$test.rmse.std
  learning_curves_test$iterations = 1:nrow(learning_curves)
  learning_curves_prepared = rbind(as.data.frame(learning_curves_train),
                                   as.data.frame(learning_curves_test))
  ggplot(data = learning_curves_prepared,
         mapping = aes(x=iterations, y=rmse, group = dataset, colour=dataset)) +
    geom_errorbar(aes(ymin=rmse-2*rmse.se, ymax=rmse+2*rmse.se), width=.01, alpha=0.02) +
    geom_line() + coord_cartesian(ylim = c(0.2, 0.5))

}

plot_learning_curve(history)
```

Feature importance
------------------
```{r}
imp = xgb.importance(colnames(train_matrix), model = model)
```

```{r, fig.width=7, fig.height=10}
xgb.plot.importance(imp)

```

Train on full dataset
------------------------
```{r}
dtrain_final <- xgb.DMatrix(full_train_matrix, label = preparedFullTarget)

model_final = xgboost(params = xgbControl, data = dtrain_final, 
                      nround=2000, nthread = 4, print.every.n = 500)

history <- xgb.cv(params = xgbControl, data = dtrain_final, 
                  nround=2000, nthread = 4, print.every.n = 500, nfold = 3)

```


Predict and un-log predictions
------------------------------

```{r}
predictions = predict(model_final, test_matrix)
fixed_predictions = exp(predictions) - 1
```
Prediction vs Target density plot
---------------------------------
```{r}
test_predictions = sample(predictions, length(validate_predictions))
count_value = data.frame(validate_predictions, preparedValidateTarget, test_predictions)
count_value = melt(count_value)
ggplot(count_value, aes(group = variable, color = variable, x = value)) + geom_density()
```

Write the result
----------------
```{r}
result = cbind(as.character(test$datetime), fixed_predictions) %>% as.data.frame()
names(result) = c('datetime', 'count')
write.csv(result, 'submission.csv', quote = F, row.names = F)
```